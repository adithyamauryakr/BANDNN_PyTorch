{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyamauryakr/BANDNN_pytorch/blob/main/hpt-BANDNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Cb7maf8QHrH9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb7maf8QHrH9",
        "outputId": "1e88e55b-5eda-4e38-f7d8-f62fd76dab01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2024.9.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Cloning into 'BANDNN_pytorch'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 45 (delta 22), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (45/45), 261.11 KiB | 3.26 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "fatal: destination path 'ANI1_dataset' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit\n",
        "!pip install optuna\n",
        "!git clone https://github.com/adithyamauryakr/BANDNN_pytorch.git\n",
        "!git clone https://github.com/isayev/ANI1_dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4giUy9H4HrWi",
      "metadata": {
        "id": "4giUy9H4HrWi"
      },
      "outputs": [],
      "source": [
        "!export PYTHONPATH=\"${PYTHONPATH}:/content/ANI1_dataset/readers/lib to PYTHONPATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "QoIjUlczHzAs",
      "metadata": {
        "id": "QoIjUlczHzAs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import lib.pyanitools as pya\n",
        "import os\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "lcQIDX3UH3EY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcQIDX3UH3EY",
        "outputId": "1c9ccf67-ad4e-4a71-cc6d-cc1d85d2785b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141sylRLH5Ce",
      "metadata": {
        "id": "141sylRLH5Ce"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "features_list = []\n",
        "with h5py.File('/content/drive/MyDrive/bandnn_datasets/molecules.h5', 'r') as h5f:\n",
        "    for mol_key in h5f.keys():\n",
        "        group = h5f[mol_key]\n",
        "        mol_data = {key: group[key][()] for key in group}\n",
        "        for k, v in mol_data.items():\n",
        "            if isinstance(v, bytes):\n",
        "                mol_data[k] = v.decode('utf-8')\n",
        "        features_list.append(mol_data)\n",
        "\n",
        "print(len(features_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "slM8RpLbH9gI",
      "metadata": {
        "id": "slM8RpLbH9gI"
      },
      "outputs": [],
      "source": [
        "y = pd.read_csv('/content/drive/MyDrive/bandnn_datasets/energy_list.csv').values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "saKzQLmEIPmH",
      "metadata": {
        "id": "saKzQLmEIPmH"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_list, y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "C861eYZEIB6j",
      "metadata": {
        "id": "C861eYZEIB6j"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, targets):\n",
        "    self.features = features\n",
        "    self.targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.targets[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9rISVpVfIKVk",
      "metadata": {
        "id": "9rISVpVfIKVk"
      },
      "outputs": [],
      "source": [
        "def collate_Fn(batch):\n",
        "    feature_batch, target_batch = zip(*batch)\n",
        "    return list(feature_batch), torch.tensor(target_batch, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lZBLJs4pIMbd",
      "metadata": {
        "id": "lZBLJs4pIMbd"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32,collate_fn=collate_Fn, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_Fn, shuffle=False, pin_memory=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8idNCah7Mo5M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8idNCah7Mo5M",
        "outputId": "dc1add57-1ade-4ae2-e452-1799fdca2449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a2ad2d3d190>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ub6U_4-VIU-n",
      "metadata": {
        "id": "ub6U_4-VIU-n"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "class BANDNN(nn.Module):\n",
        "\n",
        "    def __init__(self, bonds_input_dim, angles_input_dim, nonbonds_input_dim, dihedral_input_dim,\n",
        "                 num_hidden_layers, neurons_per_layer_bonds, neurons_per_layer_angles,\n",
        "                 neurons_per_layer_nonbonds, neurons_per_layer_dihedrals):\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      bonds_model_layers = []\n",
        "\n",
        "      for i in range(num_hidden_layers):\n",
        "\n",
        "        bonds_model_layers.append(nn.Linear(bonds_input_dim, neurons_per_layer_bonds))\n",
        "        bonds_model_layers.append(nn.ReLU())\n",
        "        bonds_input_dim = neurons_per_layer_bonds\n",
        "\n",
        "      bonds_model_layers.append(nn.Linear(neurons_per_layer_bonds, 1))\n",
        "\n",
        "      angles_layers = []\n",
        "\n",
        "      for i in range(num_hidden_layers):\n",
        "\n",
        "        angles_layers.append(nn.Linear(anlges_input_dim, neurons_per_layer_angles))\n",
        "        angles_layers.append(nn.ReLU())\n",
        "        anlges_input_dim = neurons_per_layer_angles\n",
        "\n",
        "      angles_layers.append(nn.Linear(neurons_per_layer_angles, 1))\n",
        "\n",
        "      nonbonds_layers = []\n",
        "\n",
        "      for i in range(num_hidden_layers):\n",
        "\n",
        "        nonbonds_layers.append(nn.Linear(nonbonds_input_dim, neurons_per_layer_nonbonds))\n",
        "        nonbonds_layers.append(nn.ReLU())\n",
        "        nonbonds_input_dim = neurons_per_layer_nonbonds\n",
        "\n",
        "      nonbonds_layers.append(nn.Linear(neurons_per_layer_nonbonds, 1))\n",
        "\n",
        "      dihedrals_layers = []\n",
        "\n",
        "      for i in range(num_hidden_layers):\n",
        "\n",
        "        dihedrals_layers.append(nn.Linear(dihedrals_input_dim, neurons_per_layer_dihedrals))\n",
        "        dihedrals_layers.append(nn.ReLU())\n",
        "        dihedrals_input_dim = neurons_per_layer_dihedrals\n",
        "\n",
        "      dihedrals_layers.append(nn.Linear(neurons_per_layer_dihedrals, 1))\n",
        "\n",
        "      self.bonds_model = nn.Sequential(*bonds_model_layers)\n",
        "      self.angles_model = nn.Sequential(*angles_layers)\n",
        "      self.nonbonds_model = nn.Sequential(*nonbonds_layers)\n",
        "      self.dihedrals_model = nn.Sequential(*dihedrals_layers)\n",
        "\n",
        "\n",
        "    def forward(self, bonds_input, angles_input, non_bonds_input, dihedrals_input):\n",
        "        bonds_energy = self.bonds_model(bonds_input).sum()\n",
        "        angles_energy = self.angles_model(angles_input).sum()\n",
        "        nonbonds_energy = self.nonbonds_model(non_bonds_input).sum()\n",
        "        dihedrals_energy = self.dihedrals_model(dihedrals_input).sum()\n",
        "\n",
        "        total_energy = bonds_energy + angles_energy + nonbonds_energy + dihedrals_energy\n",
        "        return total_energy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jtW7U8PIj_0",
      "metadata": {
        "id": "3jtW7U8PIj_0"
      },
      "outputs": [],
      "source": [
        "# objective function:\n",
        "def objective(trial):\n",
        "\n",
        "  #next HP values from the search space\n",
        "  num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 5)\n",
        "\n",
        "  neurons_per_layer_bonds = trial.suggest_int('neurons_per_layer', 8, 512, step=8)\n",
        "  neurons_per_layer_angles = trial.suggest_int('neurons_per_layer', 8, 512, step=8)\n",
        "  neurons_per_layer_nonbonds = trial.suggest_int('neurons_per_layer', 8, 512, step=8)\n",
        "  neurons_per_layer_dihedrals = trial.suggest_int('neurons_per_layer', 8, 512, step=8)\n",
        "\n",
        "  epochs = trial.suggest_int('epochs', 10, 50, step=10)\n",
        "  learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "  batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "  optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
        "  weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, pin_memory=True)\n",
        "  test_loader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "  #model init\n",
        "  BONDS_DIM, ANGLES_DIM, NONBONDS_DIM, DIHEDRALS_DIM = 17, 27, 17, 38\n",
        "\n",
        "  model = BANDNN(BONDS_DIM, ANGLES_DIM, NONBONDS_DIM, DIHEDRALS_DIM)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "  #optimizer selection\n",
        "\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  elif optimizer_name == 'RMSprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  #training loop\n",
        "  for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}')\n",
        "    total_epoch_loss = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "\n",
        "      for feature_dict, target in zip(*batch):\n",
        "\n",
        "        bond_feat = torch.stack([torch.tensor(arr, dtype=torch.float32) for arr in feature_dict['bonds']]).float().to(device)\n",
        "        angle_feat = torch.stack([torch.tensor(arr, dtype=torch.float32) for arr in feature_dict['angles']]).float().to(device)\n",
        "        nonbond_feat = torch.stack([torch.tensor(arr, dtype=torch.float32) for arr in feature_dict['nonbonds']]).float().to(device)\n",
        "        dihedral_feat = torch.stack([torch.tensor(arr, dtype=torch.float32) for arr in feature_dict['dihedrals']]).float().to(device)\n",
        "        energy_feat = torch.tensor([target], dtype=torch.float32).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(bond_feat, angle_feat, nonbond_feat, dihedral_feat,\n",
        "                        )\n",
        "\n",
        "        loss = criterion(outputs, energy_feat)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_samples+=1\n",
        "        total_epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    avg_loss = total_epoch_loss / num_samples\n",
        "    print(f'Average epoch Loss: {avg_loss:.4f}')\n",
        "\n",
        "  #evaluation\n",
        "    model.eval()  # Set model to eval mode\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    predictions = []\n",
        "    targets_list = []\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        for batch in test_loader:\n",
        "            features_list, targets = batch\n",
        "\n",
        "            for feature, target in zip(features_list, targets):\n",
        "                # Convert and move feature components to device\n",
        "                bonds = torch.stack([torch.tensor(b, dtype=torch.float32) for b in feature['bonds']]).to(device)\n",
        "                angles = torch.stack([torch.tensor(a, dtype=torch.float32) for a in feature['angles']]).to(device)\n",
        "                nonbonds = torch.stack([torch.tensor(n, dtype=torch.float32) for n in feature['nonbonds']]).to(device)\n",
        "                dihedrals = torch.stack([torch.tensor(d, dtype=torch.float32) for d in feature['dihedrals']]).to(device)\n",
        "\n",
        "                target = torch.tensor(target, dtype=torch.float32).to(device)\n",
        "\n",
        "                # Get model output\n",
        "                output = model(bonds, angles, nonbonds, dihedrals)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(output, target)\n",
        "                total_loss += loss.item()\n",
        "                total_samples += 1\n",
        "\n",
        "                predictions.append(output.item())\n",
        "                targets_list.append(target.item())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    print(f\"Evaluation MSE Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "  return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yoASATi45NjT",
      "metadata": {
        "id": "yoASATi45NjT"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V0R0q7SZ5S1s",
      "metadata": {
        "id": "V0R0q7SZ5S1s"
      },
      "outputs": [],
      "source": [
        "study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7KocTkif5Y4y",
      "metadata": {
        "id": "7KocTkif5Y4y"
      },
      "outputs": [],
      "source": [
        "study.best_value"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "matsci_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
